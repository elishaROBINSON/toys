{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1ztQQGtwOW9BOFm-znjyqJGIb75Fi4BcQ",
      "authorship_tag": "ABX9TyOgYa9FMyJxlKnVGJGAAv9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elishaROBINSON/toys/blob/master/auto_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pytube pydub numpy wavio tqdm wave\n",
        "%pip install -q git+https://github.com/facebookresearch/seamless_communication\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "import torchaudio\n",
        "import wave\n",
        "from seamless_communication.models.inference import Translator\n",
        "from fairseq2.memory import MemoryBlock\n",
        "from pydub import AudioSegment\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "from fairseq2.memory import MemoryBlock\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.io import StreamReader\n",
        "from pytube import YouTube\n",
        "import os\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s %(levelname)s -- %(name)s: %(message)s\",\n",
        ")\n",
        "url = \"https://www.youtube.com/watch?v=lQDcmJZggqM\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def Download(link):\n",
        "    youtubeObject = YouTube(link)\n",
        "    youtubeObject = youtubeObject.streams.get_highest_resolution()\n",
        "    try:\n",
        "        youtubeObject.download(filename=\"input.mp4\")\n",
        "    except:\n",
        "        print(\"An error has occurred\")\n",
        "    print(\"Download is completed successfully\")\n",
        "\n",
        "Download()\n",
        "\n",
        "os.system('ffmpeg -i input.mp4 -vn -acodec pcm_s16le -ar 44100 -ac 2 test_file.wav')\n",
        "waveform, sample_rate = torchaudio.load(\"test_file.wav\")\n",
        "\n",
        "def translate(wav_data):\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        dtype = torch.float16\n",
        "        logger.info(f\"Running inference on the GPU in {dtype}.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        dtype = torch.float32\n",
        "        logger.info(f\"Running inference on the CPU in {dtype}.\")\n",
        "\n",
        "    translator = Translator('seamlessM4T_large', 'vocoder_36langs', device, dtype)\n",
        "    translated_text, wav, sr = translator.predict(\n",
        "        wav_data,\n",
        "        \"s2st\",\n",
        "        \"eng\",\n",
        "        src_lang=None,\n",
        "        spkr=2,\n",
        "        sample_rate=sample_rate,\n",
        "        ngram_filtering=True,\n",
        "    )\n",
        "    return wav, sr\n",
        "\n",
        "chunk_duration = 25\n",
        "\n",
        "# Convert chunk duration to samples\n",
        "chunk_samples = int(chunk_duration * sample_rate)\n",
        "store = torch.empty(0,1, dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "\n",
        "for chunk in tqdm(range(0,waveform.size(1),chunk_samples)):\n",
        "  val = waveform[:,chunk:chunk+chunk_samples].to('cuda:0')\n",
        "  val = val.t()\n",
        "  wav, sr = translate(val)\n",
        "  store = torch.cat((store,wav[0].t()),dim=0)\n",
        "\n",
        "torchaudio.save(\n",
        "          f\"translated_out.wav\",\n",
        "          store.t().to(torch.float32).cpu(),\n",
        "          sample_rate=int(sr/3),\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "APz4uGs8_B4I",
        "outputId": "90066457-8d25-48ee-e91a-005d9da495d2"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/70 [00:00<?, ?it/s]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  1%|▏         | 1/70 [00:11<13:27, 11.70s/it]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  3%|▎         | 2/70 [00:21<12:00, 10.59s/it]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  4%|▍         | 3/70 [00:31<11:41, 10.47s/it]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  6%|▌         | 4/70 [00:40<10:52,  9.89s/it]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  7%|▋         | 5/70 [00:50<10:44,  9.91s/it]Using the cached checkpoint of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached tokenizer of the model 'seamlessM4T_large'. Set `force=True` to download again.\n",
            "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n",
            "  7%|▋         | 5/70 [01:00<13:04, 12.08s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-b4bdf04a1be8>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m torchaudio.save(\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0;34mf\"translated_out.wav\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'sample_rate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchaudio.save(\n",
        "          f\"translated_out.wav\",\n",
        "          store.t().to(torch.float32).cpu(),\n",
        "          sample_rate=int(sr/3),\n",
        "      )"
      ],
      "metadata": {
        "id": "KQqJwSU-Cq44"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC89wFQ8D2xW",
        "outputId": "be38627f-a48e-4509-800f-d2ef7ba64884"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44100"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}